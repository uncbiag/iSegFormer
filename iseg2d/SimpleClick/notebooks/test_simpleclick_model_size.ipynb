{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path(sys.path[0]).parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from isegm.model.is_plainvit_model import PlainVitModel\n",
    "from isegm.model.modeling.transformer_helper.cross_entropy_loss import CrossEntropyLoss\n",
    "\n",
    "\n",
    "model_folder = '/playpen-raid2/qinliu/projects/iSegFormer/weights'\n",
    "vitb_path = model_folder + '/sbd_vitb_epoch_54.pth'\n",
    "vitl_path = model_folder + '/sbd_vitl_epoch_54.pth'\n",
    "vith_path = model_folder + '/sbd_vith_epoch_54.pth'\n",
    "\n",
    "\n",
    "def params_vit_base_224(**kwargs):\n",
    "\n",
    "    backbone_params = dict(img_size=(224, 224), patch_size=(16,16), in_chans=3,\n",
    "        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,)\n",
    "\n",
    "    neck_params = dict(in_dim=768, out_dims=[128, 256, 512, 1024],)\n",
    "\n",
    "    head_params = dict(in_channels=[128, 256, 512, 1024], in_index=[0, 1, 2, 3],\n",
    "        dropout_ratio=0.1, num_classes=1, loss_decode=CrossEntropyLoss(),\n",
    "        align_corners=False, channels=256,)\n",
    "\n",
    "    return backbone_params, neck_params, head_params\n",
    "\n",
    "\n",
    "def params_vit_base_448(**kwargs):\n",
    "\n",
    "    backbone_params = dict(img_size=(448, 448), patch_size=(16,16), in_chans=3,\n",
    "        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,)\n",
    "\n",
    "    neck_params = dict(in_dim=768, out_dims=[128, 256, 512, 1024],)\n",
    "\n",
    "    head_params = dict(in_channels=[128, 256, 512, 1024], in_index=[0, 1, 2, 3],\n",
    "        dropout_ratio=0.1, num_classes=1, loss_decode=CrossEntropyLoss(),\n",
    "        align_corners=False, channels=256,)\n",
    "\n",
    "    return backbone_params, neck_params, head_params\n",
    "\n",
    "\n",
    "def params_vit_large_448(**kwargs):\n",
    "\n",
    "    backbone_params = dict(img_size=(448, 448), patch_size=(16,16), in_chans=3,\n",
    "        embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,)\n",
    "\n",
    "    neck_params = dict(in_dim=1024, out_dims=[192, 384, 768, 1536],)\n",
    "\n",
    "    head_params = dict(in_channels=[192, 384, 768, 1536], in_index=[0, 1, 2, 3],\n",
    "        dropout_ratio=0.1, num_classes=1, loss_decode=CrossEntropyLoss(),\n",
    "        align_corners=False, channels=256,)\n",
    "\n",
    "    return backbone_params, neck_params, head_params\n",
    "\n",
    "\n",
    "def params_vit_huge_448(**kwargs):\n",
    "\n",
    "    backbone_params = dict(img_size=(448, 448), patch_size=(14,14), in_chans=3,\n",
    "        embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,)\n",
    "\n",
    "    neck_params = dict(in_dim=1280, out_dims=[240, 480, 960, 1920],)\n",
    "\n",
    "    head_params = dict(in_channels=[240, 480, 960, 1920], in_index=[0, 1, 2, 3],\n",
    "        dropout_ratio=0.1, num_classes=1, loss_decode=CrossEntropyLoss(),\n",
    "        align_corners=False, channels=256,)\n",
    "\n",
    "    return backbone_params, neck_params, head_params\n",
    "\n",
    "\n",
    "vitb_backbone_params, vitb_neck_params, vitb_head_params = params_vit_base_448()\n",
    "model_vitb = PlainVitModel(use_disks=True, norm_radius=5, with_prev_mask=True, \n",
    "    backbone_params=vitb_backbone_params, neck_params=vitb_neck_params, \n",
    "    head_params=vitb_head_params)\n",
    "\n",
    "vitl_backbone_params, vitl_neck_params, vitl_head_params = params_vit_large_448()\n",
    "model_vitl = PlainVitModel(use_disks=True, norm_radius=5, with_prev_mask=True, \n",
    "    backbone_params=vitl_backbone_params, neck_params=vitl_neck_params, \n",
    "    head_params=vitl_head_params)\n",
    "\n",
    "vith_backbone_params, vith_neck_params, vith_head_params = params_vit_huge_448()\n",
    "model_vith = PlainVitModel(use_disks=True, norm_radius=5, with_prev_mask=True, \n",
    "    backbone_params=vith_backbone_params, neck_params=vith_neck_params, \n",
    "    head_params=vith_head_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "373.9 M\n",
      "332.0 M\n",
      "36.1 M\n",
      "3.6 M\n",
      "----\n",
      "1236.7 M\n",
      "1163.2 M\n",
      "66.0 M\n",
      "4.5 M\n",
      "----\n",
      "2526.0 M\n",
      "2414.8 M\n",
      "103.1 M\n",
      "5.2 M\n"
     ]
    }
   ],
   "source": [
    "def get_params_count(model):\n",
    "    return sum(param.numel() for param in model.parameters())\n",
    "\n",
    "for model in [model_vitb, model_vitl, model_vith]:\n",
    "    print('----')\n",
    "    print('{:.1f} M'.format(get_params_count(model) * 4.0 / 1024 / 1024))\n",
    "    print('{:.1f} M'.format(get_params_count(model.backbone) * 4.0 / 1024 / 1024))\n",
    "    print('{:.1f} M'.format(get_params_count(model.neck) * 4.0 / 1024 / 1024))\n",
    "    print('{:.1f} M'.format(get_params_count(model.head) * 4.0 / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "169.77953G 96.45703M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "532.87073G 322.17658M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "1.40193T 659.38696M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "42.44488G 96.45703M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "\n",
    "# ViT-B, ViT-L, ViT-H 448\n",
    "input = torch.randn(1, 4, 448, 448)\n",
    "point = torch.randn(1, 2, 3)\n",
    "\n",
    "for model in [model_vitb, model_vitl, model_vith]:\n",
    "    model.eval()\n",
    "\n",
    "    macs, params = profile(model, inputs=(input, point))\n",
    "    gflops, params = clever_format([macs*2, params], \"%.5f\")\n",
    "\n",
    "    print(gflops, params)\n",
    "\n",
    "\n",
    "vitb_backbone_params, vitb_neck_params, vitb_head_params = params_vit_base_224()\n",
    "model_vitb = PlainVitModel(use_disks=True, norm_radius=5, with_prev_mask=True, \n",
    "    backbone_params=vitb_backbone_params, neck_params=vitb_neck_params, \n",
    "    head_params=vitb_head_params)\n",
    "\n",
    "# ViT-B-224\n",
    "input = torch.randn(1, 4, 224, 224)\n",
    "point = torch.randn(1, 2, 3)\n",
    "\n",
    "for model in [model_vitb]:\n",
    "    model.eval()\n",
    "\n",
    "    macs, params = profile(model, inputs=(input, point))\n",
    "    gflops, params = clever_format([macs*2, params], \"%.5f\")\n",
    "\n",
    "    print(gflops, params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 19:58:26) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e89c9d8a920cc6bbfac98998034c8a15ed2e75dae1678af787f4c4e38e5be518"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
